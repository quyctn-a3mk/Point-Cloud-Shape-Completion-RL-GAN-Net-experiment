{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Classes\\MachineLearning-Th.Khanh\\Report\\ReinforcementLearning\\RL-GAN\\notebooks\\train-gan.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-gan.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-gan.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-gan.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-gan.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-gan.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyntcloud import PyntCloud\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "##\n",
    "from src.autoencoder import AutoEncoder, PointcloudDatasetAE\n",
    "from src.chamferloss import ChamferLoss_distance\n",
    "from src.gan import GenSAGAN, DiscSAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "LAMBDA = 1e1\n",
    "use_cuda = torch.cuda.is_available()\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    try:\n",
    "        alpha = torch.rand(BATCH_SIZE, 1)\n",
    "        alpha = alpha.expand(real_data.size())\n",
    "        alpha = alpha.cuda() if use_cuda else alpha\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        if use_cuda:\n",
    "            interpolates = interpolates.cuda()\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "        disc_interpolates, _ = netD(interpolates)\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(\n",
    "                                    disc_interpolates.size()),\n",
    "                                create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "        return gradient_penalty\n",
    "    except:\n",
    "        print(\"Err\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/shape_net_core_uniform_samples_2048/\"\n",
    "list_point_clouds = np.load('./data/filter/list_point_cloud_filepath.npy')\n",
    "X_train, X_test, _, _ = train_test_split(list_point_clouds, list_point_clouds, test_size=0.1, random_state=42)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PointcloudDatasetAE(DATA_DIR, X_train)\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=2, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = PointcloudDatasetAE(DATA_DIR, X_test)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=2, shuffle=False, batch_size=1)\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    data = data.permute([0,2,1])\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 5\n",
    "generator = GenSAGAN(z_dim=z_dim).to(device)\n",
    "discriminator = DiscSAGAN().to(device) \n",
    "autoencoder = AutoEncoder(2048).to(device)\n",
    "chamfer_loss = ChamferLoss_distance(2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = 1.0e-4\n",
    "d_lr = 1.0e-4\n",
    "lr = 1.0e-4\n",
    "d_gp_weight = 1e1   \n",
    "momentum = 0.95\n",
    "optimizer_AE = torch.optim.Adam(autoencoder.parameters(), lr=lr, betas=(momentum, 0.999))\n",
    "g_optim = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "d_optim = torch.optim.Adam(discriminator.parameters(), lr=d_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './models/gan/'\n",
    "now =   str(datetime.datetime.now())+'z'+str(z_dim)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.makedirs(ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR + now):\n",
    "    os.makedirs(ROOT_DIR + now)\n",
    "\n",
    "LOG_DIR = ROOT_DIR + now + '/logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OUTPUTS_DIR = ROOT_DIR  + now + '/outputs/'\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.makedirs(OUTPUTS_DIR)\n",
    "\n",
    "MODEL_DIR = ROOT_DIR + now + '/models/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_state_dict(torch.load('./models/autoencoder/2022-08-06 15:19:12.904709/models/14_ae_.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(generator, autoencoder,epoch):\n",
    "    for i in tqdm.trange(5):\n",
    "        # points = PyntCloud.from_file(X_test[i])\n",
    "        # points = np.array(points.points)\n",
    "        # points_normalized = (points - (-0.5)) / (0.5 - (-0.5))\n",
    "        # points = points_normalized.astype(np.float)\n",
    "        # points = torch.from_numpy(points).unsqueeze(0)\n",
    "        # points = points.permute([0,2,1]).float().to(device)\n",
    "        # print(points.shape)\n",
    "        autoencoder.eval()\n",
    "        generator.eval()\n",
    "        z = torch.randn(1, z_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "                gen_out, _ = generator(z)\n",
    "                out_data = autoencoder.decode(gen_out)\n",
    "                # loss = chamfer_loss(out_data, points)\n",
    "        # print(loss.item())                \n",
    "        output = out_data[0,:,:]\n",
    "        output = output.permute([1,0]).detach().cpu().numpy()\n",
    "        # inputt = points[0,:,:]\n",
    "        # inputt = inputt.permute([1,0]).detach().cpu().numpy()\n",
    "        fig = plt.figure()\n",
    "        ax_x = fig.add_subplot(111, projection='3d')\n",
    "        x_ = output\n",
    "        ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "        ax_x.set_xlim([0,1])\n",
    "        ax_x.set_ylim([0,1])\n",
    "        ax_x.set_zlim([0,1])\n",
    "        fig.savefig(OUTPUTS_DIR+'/{}_{}_{}.png'.format(epoch, i, 'out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training')\n",
    "for epoch in range(1000):\n",
    "    autoencoder.train()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        data = data.permute([0,2,1]).float().to(device)\n",
    "        # optimizer_AE.zero_grad()\n",
    "        autoencoder.eval()\n",
    "        generator.train()\n",
    "        discriminator.train()      \n",
    "        with torch.no_grad():\n",
    "            gfv = autoencoder.encode(data)\n",
    "        z = torch.randn(data.shape[0], z_dim).to(device)\n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        fake_out, _ = generator(z)\n",
    "        # print(gfv.device)\n",
    "        d_fake, _ = discriminator(fake_out)\n",
    "        d_real, _ = discriminator(gfv)\n",
    "        d_loss = -(torch.mean(d_real) - torch.mean(d_fake))\n",
    "        d_grad_penalty = calc_gradient_penalty(discriminator, gfv, fake_out)\n",
    "        if not d_grad_penalty:\n",
    "            continue\n",
    "        total_d_loss = d_loss + d_grad_penalty\n",
    "        total_d_loss.backward()\n",
    "        d_optim.step()\n",
    "        #####################################\n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()     \n",
    "        g_out, _ = generator(z)        \n",
    "        d_fake, _ = discriminator(g_out)\n",
    "        gen_loss = -torch.mean(d_fake)        \n",
    "        out_data = autoencoder.decode(g_out)\n",
    "        loss = gen_loss\n",
    "        loss.backward()\n",
    "        g_optim.step()\n",
    "        print('Epoch: {}, Iteration: {},  G Loss: {:.4f} D Loss: {:.4f} '.format(epoch, i, loss.item(), total_d_loss.item()))\n",
    "        summary_writer.add_scalar('G Loss', loss.item())\n",
    "        summary_writer.add_scalar('GP  Loss', d_grad_penalty.item())\n",
    "        summary_writer.add_scalar('D Loss', d_loss.item())\n",
    "        summary_writer.add_scalar('Total D Loss', total_d_loss.item())    \n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(generator.state_dict(), MODEL_DIR+'{}_gen_.pt'.format(epoch))\n",
    "        torch.save(discriminator.state_dict(), MODEL_DIR+'{}_disc_.pt'.format(epoch))    \n",
    "    if epoch % 5 == 0:\n",
    "        test_model(generator, autoencoder, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d3067bb23fa9af4b3061e062d92fd01011ec029f6d29054d635839b701de19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
