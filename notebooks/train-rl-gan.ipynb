{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Classes\\MachineLearning-Th.Khanh\\Report\\ReinforcementLearning\\RL-GAN\\notebooks\\train-rl-gan.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "##\n",
    "from src.autoencoder import AutoEncoder, PointcloudDatasetAE, PointcloudDatasetNoisy\n",
    "from src.chamferloss import ChamferLoss_loss\n",
    "from src.gan import GenSAGAN, DiscSAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.episodes = []\n",
    "        self.buffer_size = size\n",
    "    def add_to_buffer(self, state, action, reward, next_state):\n",
    "        if len(self.episodes) == self.buffer_size:\n",
    "            self.episodes = self.episodes[1:]\n",
    "        self.episodes.append((state.detach().cpu().numpy(), action.detach().cpu().numpy(), reward.detach().cpu().numpy(), next_state.detach().cpu().numpy()))\n",
    "    def get_batch(self, batch_size=10):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_state = []\n",
    "        done = []\n",
    "        for i in range(batch_size):\n",
    "            epi = random.choice(self.episodes)\n",
    "            states.append(epi[0])\n",
    "            actions.append(epi[1])\n",
    "            rewards.append(epi[2])\n",
    "            next_state.append(epi[3])        \n",
    "        rewards = np.array(rewards)\n",
    "        rewards = rewards.reshape((rewards.shape[0],1))\n",
    "        return torch.Tensor(states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, z_shape):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = z_shape      \n",
    "        self.linear1 = nn.Linear(self.state_dim, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(400)\n",
    "        self.linear2 = nn.Linear(400 + z_shape, 300)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.linear3 = nn.Linear(300, 300)\n",
    "        self.linear4 = nn.Linear(300, 1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "    def forward(self, state, z):\n",
    "        out = (F.relu(self.linear1(state)))\n",
    "        out = (F.relu(self.linear2(torch.cat([out, z], dim=1))))\n",
    "        out = self.linear3(out)\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_dim,  z_shape, max_action=10):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = z_shape\n",
    "        self.linear1 = nn.Linear(self.state_dim, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.linear2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.linear3 = nn.Linear(400, 300)\n",
    "        self.linear4 = nn.Linear(300, self.num_actions)\n",
    "        self.max_action = max_action\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = F.leaky_relu((self.linear1(x)))\n",
    "        out = F.leaky_relu((self.linear2(out)))\n",
    "        out = F.tanh(self.linear3(out))\n",
    "        out = self.max_action * F.tanh(self.linear4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(nn.Module):\n",
    "    def __init__(self, max_action):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.actor = ActorNet(128, z_dim, max_action)\n",
    "        self.critic = CriticNet(128, z_dim)        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.replay_buffer = ReplayBuffer(int(1e6))\n",
    "    def get_optimal_action(self, state):\n",
    "        return self.actor(state)\n",
    "    def forward(self):\n",
    "        state, action, reward, next_state = self.replay_buffer.get_batch(batch_size_actor)\n",
    "        state = state[:,0,:].float()\n",
    "        next_state = next_state[:,0,:].float()\n",
    "        action = action[:,0,:].float()\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        next_state = next_state.to(device)        \n",
    "        target_q = reward\n",
    "        q_batch = self.critic(state, action)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss = F.mse_loss(q_batch, target_q)\n",
    "        value_loss.backward()        \n",
    "        self.critic_optimizer.step() \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss = - self.critic(state, self.actor(state)).mean()\n",
    "        policy_loss.backward()        \n",
    "        self.actor_optimizer.step()\n",
    "        return value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/shape_net_core_uniform_samples_2048/\"\n",
    "list_point_clouds = np.load('./data/filter/list_point_cloud_filepath.npy')\n",
    "X_train, X_test, _, _ = train_test_split(list_point_clouds, list_point_clouds, test_size=0.1, random_state=42)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = PointcloudDatasetNoisy(DATA_DIR, X_train)\n",
    "train_dataset = PointcloudDatasetAE(DATA_DIR, X_train)\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=0, shuffle=True, batch_size=1)\n",
    "train_loader_iterator = iter(train_dataloader)\n",
    "\n",
    "# test_dataset = PointcloudDatasetNoisy(DATA_DIR, X_test)\n",
    "test_dataset = PointcloudDatasetAE(DATA_DIR, X_test)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=0, shuffle=True, batch_size=1)\n",
    "test_loader_iterator = iter(test_dataloader)\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    data = data.permute([0,2,1])\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Classes\\MachineLearning-Th.Khanh\\Report\\ReinforcementLearning\\RL-GAN\\notebooks\\train-rl-gan.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000007?line=0'>1</a>\u001b[0m z_dim \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000007?line=1'>2</a>\u001b[0m autoencoder \u001b[39m=\u001b[39m AutoEncoder(\u001b[39m2048\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000007?line=2'>3</a>\u001b[0m generator \u001b[39m=\u001b[39m GenSAGAN(z_dim\u001b[39m=\u001b[39mz_dim)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000007?line=3'>4</a>\u001b[0m discriminator \u001b[39m=\u001b[39m DiscSAGAN()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "z_dim = 5\n",
    "autoencoder = AutoEncoder(2048).to(device)\n",
    "generator = GenSAGAN(z_dim=z_dim).to(device)\n",
    "discriminator = DiscSAGAN().to(device)\n",
    "\n",
    "max_action = 2\n",
    "ddpg = DDPG(max_action).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Classes\\MachineLearning-Th.Khanh\\Report\\ReinforcementLearning\\RL-GAN\\notebooks\\train-rl-gan.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000015?line=2'>3</a>\u001b[0m weights_gen \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./gan_out/2019-11-29 17:33:15.146770z5/models/980_gen_.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000015?line=3'>4</a>\u001b[0m weight_disc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./gan_out/2019-11-29 17:33:15.146770z5/models/980_disc_.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000015?line=5'>6</a>\u001b[0m autoencoder\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(weights_ae))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000015?line=6'>7</a>\u001b[0m generator\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(weights_gen))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Classes/MachineLearning-Th.Khanh/Report/ReinforcementLearning/RL-GAN/notebooks/train-rl-gan.ipynb#ch0000015?line=7'>8</a>\u001b[0m discriminator\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(weight_disc))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "# weights_ae = './models/autoencoder/2022-08-06 15:19:12.904709/models/14_ae_.pt'\n",
    "weights_ae = './models/autoencoder/990_ae_.pt'\n",
    "\n",
    "# weights_gen = './model/gan/2022-08-06 21:43:44.412123z5/models/200_gen_.pt'\n",
    "# weight_disc = './model/gan/2022-08-06 21:43:44.412123z5/models/200_disc_.pt'\n",
    "# weights_gen = './models/gan/2022-08-06 21:43:44.412123z5/models/980_gen_.pt'\n",
    "# weight_disc = './models/gan/2022-08-07 05:05:01.588883z5/models/980_disc_.pt'\n",
    "weights_gen = './models/gan/980_gen_.pt'\n",
    "weight_disc = './models/gan/980_disc_.pt'\n",
    "\n",
    "autoencoder.load_state_dict(torch.load(weights_ae))\n",
    "generator.load_state_dict(torch.load(weights_gen))\n",
    "discriminator.load_state_dict(torch.load(weight_disc))\n",
    "\n",
    "autoencoder.eval()\n",
    "generator.eval()\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1e6\n",
    "batch_size_actor = 100\n",
    "start_time = 1e3\n",
    "\n",
    "chamferloss = ChamferLoss_loss(2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './models/rl-gan/'\n",
    "now =   str(datetime.datetime.now())+'_start_4_max_2_f'\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.makedirs(ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR + now):\n",
    "    os.makedirs(ROOT_DIR + now)\n",
    "\n",
    "LOG_DIR = ROOT_DIR + now + '/logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OUTPUTS_DIR = ROOT_DIR  + now + '/outputs/'\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.makedirs(OUTPUTS_DIR)\n",
    "\n",
    "MODEL_DIR = ROOT_DIR + now + '/models/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsteps in range(0,int(max_steps)):\n",
    "    try:\n",
    "        data = next(train_loader_iterator)\n",
    "    except StopIteration:\n",
    "        train_loader_iterator = iter(train_dataloader)\n",
    "        data = next(train_loader_iterator)\n",
    "    data = data.permute([0,2,1]).float().to(device)            \n",
    "    if tsteps != 0:\n",
    "        losses = ddpg()            \n",
    "    state_t = autoencoder.encode(data)    \n",
    "    if tsteps < start_time:\n",
    "        action_t = -2 * max_action * torch.rand(1, z_dim) + max_action\n",
    "        action_t = action_t.to(device)\n",
    "    else:\n",
    "        action_t = (ddpg.get_optimal_action(state_t).detach() + 0.1 * torch.randn(1, z_dim).to(device)).clamp(-max_action, max_action)\n",
    "#         if tsteps % 10000 == 0:\n",
    "#             max_action += 1\n",
    "#             ddpg.actor.max_action = max_action\n",
    "    next_state, _ = generator(action_t)    \n",
    "    reward_gfv = -F.mse_loss(next_state, state_t)\n",
    "    reward_chamfer = -chamferloss(autoencoder.decode(next_state), autoencoder.decode(state_t))\n",
    "    reward_disc, _ = discriminator(next_state)\n",
    "    reward_disc = torch.mean(reward_disc)\n",
    "    # reward = reward_gfv * 0.01 + reward_chamfer * 5 + reward_disc * 0.05 + (-torch.norm(action_t)) * 0.1\n",
    "    reward = reward_gfv * 0.1 + reward_chamfer * 5.0 + reward_disc * 0.1 + (-torch.norm(action_t)) * 0.1\n",
    "    # reward = reward_gfv * 0.1 + reward_chamfer * 5.0 + reward_disc * 0.1 + (-torch.norm(action_t)) * 0.05\n",
    "#     reward = reward_gfv * 0.1 + reward_chamfer * 5.0 + reward_disc * 0.1    \n",
    "    ddpg.replay_buffer.add_to_buffer(state_t, action_t, reward, next_state)\n",
    "    if tsteps % 10:\n",
    "        print('Iter : {}, Reward : {:.4f}, GFV: {:.4f}, Chamfer: {:.4f}, Disc: {:.4f}, Action: {}'.format(tsteps, reward, reward_gfv, reward_chamfer, reward_disc, action_t))\n",
    "    summary_writer.add_scalar('train total reward', reward)\n",
    "    summary_writer.add_scalar('train gfv rewards', reward_gfv)\n",
    "    summary_writer.add_scalar('train reward_chamfer', reward_chamfer)\n",
    "    summary_writer.add_scalar('train reward_disc', reward_disc)\n",
    "    # if tsteps % 1 == 0 and tsteps > start_time:\n",
    "    if tsteps % 1000 <= 10 and tsteps > start_time:\n",
    "        optimal_action = ddpg.get_optimal_action(state_t).detach()\n",
    "        new_state, _ = generator(optimal_action)        \n",
    "        out_data = autoencoder.decode(new_state)\n",
    "        output = out_data[0,:,:]\n",
    "        output = output.permute([1,0]).detach().cpu().numpy()\n",
    "        fig = plt.figure()\n",
    "        ax_x = fig.add_subplot(111, projection='3d')\n",
    "        x_ = output\n",
    "        ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "        ax_x.set_xlim([0,1])\n",
    "        ax_x.set_ylim([0,1])\n",
    "        ax_x.set_zlim([0,1])\n",
    "        fig.savefig(OUTPUTS_DIR+'/{}_{}_{}.png'.format(tsteps, i, 'val_out'))\n",
    "        output = autoencoder.decode(state_t)\n",
    "        output = output[0,:,:]\n",
    "        output = output.permute([1,0]).detach().cpu().numpy()\n",
    "        fig = plt.figure()\n",
    "        ax_x = fig.add_subplot(111, projection='3d')\n",
    "        x_ = output\n",
    "        ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "        ax_x.set_xlim([0,1])\n",
    "        ax_x.set_ylim([0,1])\n",
    "        ax_x.set_zlim([0,1])\n",
    "        fig.savefig(OUTPUTS_DIR+'/{}_{}_{}.png'.format(tsteps, i, 'val_in'))\n",
    "        plt.close('all')\n",
    "        torch.save(ddpg.state_dict(), MODEL_DIR+'{}_ddpg_.pt'.format(tsteps))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ddpg.state_dict(), MODEL_DIR+'{}_ddpg_.pt'.format('final'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d3067bb23fa9af4b3061e062d92fd01011ec029f6d29054d635839b701de19f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
